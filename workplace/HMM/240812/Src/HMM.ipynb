{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood on Test Set: -24514.15051202856\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.92      0.92      2114\n",
      "         1.0       0.20      0.24      0.22       186\n",
      "\n",
      "    accuracy                           0.86      2300\n",
      "   macro avg       0.57      0.58      0.57      2300\n",
      "weighted avg       0.87      0.86      0.87      2300\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1937  177]\n",
      " [ 142   44]]\n",
      "AUC-ROC: 0.576415804518774\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hmmlearn import hmm\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Load the cleaned dataset\n",
    "data_cleaned = pd.read_csv('../../../Data_processing/Outputs/train.csv')\n",
    "\n",
    "# data_cleaned = data_cleaned.loc[data_cleaned['usubjid'].isin(data_cleaned[data_cleaned['label'] == 1][\"usubjid\"].unique())]\n",
    "data_cleaned.dropna(inplace=True)   \n",
    "\n",
    " #['full', 'diag', 'tied', 'spherical']\n",
    "# Apply global min-max normalization to the specified columns\n",
    "# scaler = MinMaxScaler()\n",
    "# data_cleaned[['aval_AlloMap', 'aval_AlloSure']] = scaler.fit_transform(data_cleaned[['aval_AlloMap', 'aval_AlloSure']])\n",
    "\n",
    "# def minmax_scaler_by_id(df, columns, id_column):\n",
    "#     df_scaled = df.copy()\n",
    "#     scaler = MinMaxScaler()\n",
    "\n",
    "#     # Apply MinMaxScaler for each group of 'usubjid'\n",
    "#     df_scaled[columns] = df.groupby(id_column)[columns].apply(lambda x: pd.DataFrame(scaler.fit_transform(x), columns=columns, index=x.index))\n",
    "    \n",
    "#     return df_scaled\n",
    "\n",
    "# # Apply the scaler at the ID level\n",
    "# data_cleaned = minmax_scaler_by_id(data_cleaned, ['aval_AlloMap', 'aval_AlloSure', \"ady_dna\"], 'usubjid')\n",
    "\n",
    "# Group data by 'usubjid' to maintain sequence structure\n",
    "grouped_data = data_cleaned.groupby('usubjid')\n",
    "\n",
    "# Shuffle\n",
    "shuffled_df = grouped_data.sample(frac=1, random_state=42)\n",
    "shuffled_df = shuffled_df.sort_values(['usubjid', 'ady_dna'])\n",
    "\n",
    "# Reset the index of the shuffled dataframe\n",
    "grouped_data = shuffled_df.reset_index(drop=True)\n",
    "grouped_data = grouped_data.groupby('usubjid')\n",
    "\n",
    "X_grouped = [group[['aval_AlloSure', 'aval_AlloMap', \"ady_dna\"]].values for name, group in grouped_data]\n",
    "y_grouped = [group['label'].values for name, group in grouped_data]\n",
    "\n",
    "\n",
    "# Calculate the split index for 80%/20%\n",
    "split_index = int(0.80 * len(X_grouped))\n",
    "\n",
    "# Split the sequences into training and testing sets\n",
    "X_train_grouped = X_grouped[:split_index]\n",
    "X_test_grouped = X_grouped[split_index:]\n",
    "y_train_grouped = y_grouped[:split_index]\n",
    "y_test_grouped = y_grouped[split_index:]\n",
    "\n",
    "# Concatenate the sequences to form the training and testing sets\n",
    "X_train = np.concatenate(X_train_grouped)\n",
    "X_test = np.concatenate(X_test_grouped)\n",
    "y_train = np.concatenate(y_train_grouped)\n",
    "y_test = np.concatenate(y_test_grouped)\n",
    "\n",
    "# Prepare lengths for the sequences\n",
    "train_lengths = [len(x) for x in X_train_grouped]\n",
    "test_lengths = [len(x) for x in X_test_grouped]\n",
    "\n",
    "# Initialize and train the HMM model\n",
    "model = hmm.GaussianHMM(n_components=2, covariance_type=\"tied\", n_iter=50000, random_state=42, init_params='stmc')\n",
    "\n",
    "\n",
    "model.fit(X_train, lengths=train_lengths)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "log_likelihood = model.score(X_test, lengths=test_lengths)\n",
    "print(f\"Log-Likelihood on Test Set: {log_likelihood}\")\n",
    "\n",
    "# Predict the hidden states for the testing set\n",
    "predicted_hidden_states = model.predict(X_test, lengths=test_lengths)\n",
    "\n",
    "# Remove NaNs from y_test for proper evaluation\n",
    "y_test_clean = y_test[~np.isnan(y_test)]\n",
    "predicted_hidden_states_clean = predicted_hidden_states[~np.isnan(y_test)]\n",
    "\n",
    "# Generate and print classification metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_clean, predicted_hidden_states_clean))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test_clean, predicted_hidden_states_clean))\n",
    "\n",
    "# If it's a binary classification, you can also calculate AUC-ROC\n",
    "if len(np.unique(y_test_clean)) == 2:\n",
    "    auc = roc_auc_score(y_test_clean, predicted_hidden_states_clean)\n",
    "    print(f\"AUC-ROC: {auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
